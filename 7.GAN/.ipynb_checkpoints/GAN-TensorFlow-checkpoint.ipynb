{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297e296f",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "\n",
    "### 작성자: 고려대학교 수학과 석사과정 최선묵\n",
    "\n",
    "[References]  \n",
    "[Paper Link](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76463fb3",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986b184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f995fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, Reshape\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955ffdf",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddff9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_image(batch_size=32):\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "    \n",
    "    # Add the color channel - change to 4D tensor, and convert the data type to 'float32'\n",
    "    train_images = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "    \n",
    "    # Set the pixel values from -1 to 1\n",
    "    train_images = (train_images/255.0) * 2 - 1\n",
    "    \n",
    "    # Shuffle and separate in batch\n",
    "    buffer_size = train_images.shape[0]\n",
    "    train_images_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size).batch(batch_size)\n",
    "    \n",
    "    return train_images_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4edd06",
   "metadata": {},
   "source": [
    "### Hyperparameters for latent codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c69452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise\n",
    "noise_dim = 62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86f5a3",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cba7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(n_filters=128, input_size=noise_dim):\n",
    "    # Build functional API model\n",
    "    # input\n",
    "    input_tensor = Input(shape=(input_size, ))\n",
    "\n",
    "    # Fully-connected layer.\n",
    "    x = Dense(units=1024, use_bias=False, kernel_initializer='he_uniform') (input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    # Fully-connected layer. The output should be able to reshape into 7x7\n",
    "    x = Dense(units=7*7*128, use_bias=False, kernel_initializer='he_uniform') (x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    # Reshape\n",
    "    x = Reshape(target_shape=(7, 7, 128))(x)\n",
    "\n",
    "    nf = n_filters\n",
    "    # First transposed convolutional layer\n",
    "\n",
    "    x = Conv2DTranspose(nf, kernel_size=(4, 4), strides=(2, 2), padding='same', \n",
    "                        use_bias=False, kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Number of filters halved after each transposed convolutional layer\n",
    "    nf = nf//2\n",
    "    \n",
    "    # Second transposed convolutional layer\n",
    "    # strides=(2, 2): shape is doubled after the transposed convolution\n",
    "    x = Conv2DTranspose(nf, kernel_size=(4, 4), strides=(2, 2), padding='same', \n",
    "                        use_bias=False, kernel_initializer='he_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Final transposed convolutional layer: output shape: 28x28x1, tanh activation\n",
    "    output = Conv2DTranspose(1, kernel_size=(4, 4), strides=(1, 1), padding=\"same\", \n",
    "                             activation=\"tanh\", kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a58c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 62)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              63488     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6272)              6422528   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262144    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 64)        131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         1025      \n",
      "=================================================================\n",
      "Total params: 6,910,209\n",
      "Trainable params: 6,895,233\n",
      "Non-trainable params: 14,976\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 23:23:19.728567: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-04 23:23:19.728848: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "g_model = create_generator()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b53bc",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13af48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(n_filters=64, input_shape=(28, 28, 1)):\n",
    "    # Build functional API model\n",
    "    # Image Input\n",
    "    image_input = Input(shape=input_shape)\n",
    "\n",
    "    nf = n_filters\n",
    "    \n",
    "    x = Conv2D(nf, kernel_size=(4, 4), strides=(2, 2), padding=\"same\", use_bias=True)(image_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Number of filters doubled after each convolutional layer\n",
    "    nf = nf*2\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    # Output shape: 7x7\n",
    "    x = Conv2D(nf, kernel_size=(4, 4), strides=(2, 2), padding=\"same\", use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Flatten the convolutional layers\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # FC layer\n",
    "    x = Dense(1024, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # Discriminator output. Sigmoid activation function to classify \"True\" or \"False\"\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Discriminator model (not compiled)\n",
    "    d_model = Model(inputs=image_input, outputs=output)\n",
    "    \n",
    "    return d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ffd7b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        1088      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              6422528   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 6,560,577\n",
      "Trainable params: 6,558,145\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_model = create_discriminator()\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4195d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, d_model, g_model, noise_size=noise_dim, d_iter=1, seed=None):\n",
    "        super(GAN, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.g_model = g_model\n",
    "        self.noise_size = noise_size\n",
    "        self.d_iter = d_iter\n",
    "        self.seed = seed\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "\n",
    "    def create_gen_input(self, batch_size, seed):\n",
    "        noise = tf.random.normal([batch_size, self.noise_size], seed=seed)\n",
    "        return noise\n",
    "\n",
    "    def train_step(self, real_image_batch):\n",
    "        \n",
    "        # Define loss functions\n",
    "        binary_loss = BinaryCrossentropy()\n",
    "#         categorical_loss = CategoricalCrossentropy()\n",
    "        \n",
    "        # Half-batch for training discriminator and batch for training generator and auxiliary model\n",
    "        batch_size = tf.shape(real_image_batch)[0]\n",
    "        \n",
    "        # Create generator input \n",
    "        g_noise = self.create_gen_input(batch_size=batch_size, seed=self.seed)\n",
    "#         g_input = self.concat_inputs([g_cat, g_conti, g_noise])\n",
    "        \n",
    "        for _ in range(self.d_iter):\n",
    "            with tf.GradientTape() as d_tape: \n",
    "                self.d_model.trainable = True\n",
    "                d_tape.watch(self.d_model.trainable_variables)\n",
    "\n",
    "                # Train discriminator using half batch real images. Real images have labels 1.\n",
    "                y_disc_real = tf.ones((batch_size, 1))\n",
    "                d_real_output = self.d_model(real_image_batch, training=True)\n",
    "                d_loss_real = binary_loss(y_disc_real, d_real_output)\n",
    "\n",
    "                # Train discriminator using half batch fake images. Fake images have labels 0. \n",
    "                y_disc_fake = tf.zeros((batch_size, 1))\n",
    "\n",
    "                # Create fake image batch\n",
    "                fake_image_batch = self.g_model(g_noise, training=True)\n",
    "                d_fake_output = self.d_model(fake_image_batch, training=True)\n",
    "                d_loss_fake = binary_loss(y_disc_fake, d_fake_output)\n",
    "\n",
    "                # Total Loss of Discriminator \n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            # Calculate gradients\n",
    "            d_gradients = d_tape.gradient(d_loss, self.d_model.trainable_variables)\n",
    "\n",
    "            # Optimize\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradients, self.d_model.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # Create generator input \n",
    "            g_noise = self.create_gen_input(batch_size=batch_size*2, seed=self.seed)\n",
    "            \n",
    "            g_tape.watch(self.g_model.trainable_variables)\n",
    "            \n",
    "            # Create fake image batch\n",
    "            fake_image_batch = self.g_model(g_noise, training=True)\n",
    "            d_fake_output = self.d_model(fake_image_batch, training=True)\n",
    "            \n",
    "            # Generator Image loss\n",
    "            y_gen_fake = tf.ones((batch_size*2, 1))\n",
    "            \n",
    "            g_loss = binary_loss(y_gen_fake, d_fake_output)\n",
    "            \n",
    "            \n",
    "        # Calculate gradients\n",
    "        # We do not want to modify the neurons in the discriminator when training the generator and the auxiliary model\n",
    "        self.d_model.trainable=False\n",
    "        g_gradients = g_tape.gradient(g_loss, self.g_model.trainable_variables)\n",
    "        \n",
    "        # Optimize\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.g_model.trainable_variables))\n",
    "\n",
    "        return {\"d_loss_real\": d_loss_real, \"d_loss_fake\": d_loss_fake, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f62fddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 23:32:17.036239: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19/1875 [..............................] - ETA: 7:40 - d_loss_real: 0.7773 - d_loss_fake: 0.7609 - g_loss: 0.7983"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j_/45z7qrp57fq6gc7pn9bn7n_40000gn/T/ipykernel_64441/3897231134.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_real_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/infoGAN/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoGAN/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoGAN/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoGAN/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoGAN/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoGAN/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/infoGAN/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan = GAN(d_model, g_model)\n",
    "\n",
    "gan.compile(d_optimizer=Adam(learning_rate=2e-4),\n",
    "            g_optimizer=Adam(learning_rate=5e-4))\n",
    "\n",
    "real_images = load_real_image(batch_size=32)\n",
    "\n",
    "history = gan.fit(real_images, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9be4550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/generator/assets\n",
      "INFO:tensorflow:Assets written to: saved_model/discriminator/assets\n",
      "INFO:tensorflow:Assets written to: saved_model/q_network/assets\n"
     ]
    }
   ],
   "source": [
    "g_model.save('saved_models/generator')\n",
    "d_model.save('saved_models/discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bf1bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 21:06:21.509448: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-04 21:06:21.509652: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "g_model = tf.keras.models.load_model('saved_models/generator')\n",
    "d_model = tf.keras.models.load_model('saved_models/discriminator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9291ab",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "146a89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "078419a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([10, 62], seed=831)\n",
    "\n",
    "x = g_model(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13d5917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_input = Concatenate()([cat_code, conti_code, noise])\n",
    "\n",
    "x = g_model(g_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05e629c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j_/45z7qrp57fq6gc7pn9bn7n_40000gn/T/ipykernel_64441/1130284344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m15.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "\n",
    "rows = 2\n",
    "columns = 5\n",
    "\n",
    "for i in range(10) : \n",
    "    image_index = i      # image index \n",
    "    title = \"Sample {}\".format(image_index) # image title\n",
    "    plt.subplot(rows, columns, image_index+1) # subplot \n",
    "    plt.title(title)   # title \n",
    "    # // plt.axis('off')\n",
    "    plt.xticks([])  # x = None \n",
    "    plt.yticks([])  # y = None\n",
    "    plt.imshow(x[i], cmap=plt.get_cmap('gray'))  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41b027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4e322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infogan\n",
   "language": "python",
   "name": "infogan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
