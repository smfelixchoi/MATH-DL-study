{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e198236b",
   "metadata": {},
   "source": [
    "# Solving Linear Regression \n",
    "\n",
    "- Only Training set, No Test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7776a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c61f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    data_load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c039be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4183967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3360, 4) (3360,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74133960",
   "metadata": {},
   "source": [
    "### Analytic Solution\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{N \\times d}$ be the ***design matrix*** of the data,\n",
    "that is, the $i$th **row vector** of $X$ is $\\hat{x^i} = (1, x^i)$.\n",
    "\n",
    "Let $y \\in \\mathbb{R}^N$ be the **row vector** consisting of labels of data.\n",
    "\n",
    "Then, the loss function $L(w)$ can be written as the following vector notation:\n",
    "$$L(w) = \\frac{1}{2N}\\sum_{i=1}^N (y_i- w x_i^\\top)^2=\\frac{1}{2N}(y - w X^\\top) (y - wX^\\top)^\\top.$$\n",
    "\n",
    "Since the loss function is convex w.r.t $w$, we can find the minimum by differentiating the function w.r.t $w$.\n",
    "\n",
    "$$\\nabla_{w} L(w) = -yX + w(X^\\top X)$$\n",
    "\n",
    "Therefore, if $X^\\top X$ is invertible, the analytic optimal solution is\n",
    "$$ \\hat{w} = yX(X^\\top X)^{-1}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485d2bb",
   "metadata": {},
   "source": [
    "```np.linalg.solve(A,b)```\n",
    "\n",
    "- It finds a solution x for the linear equation Ax = b.\n",
    "- Here, x is considered to be a column vector. Thus we take transpose to the equation above.\n",
    "\n",
    "$$\\nabla_{w} L(w) = 0 \\quad \\Leftrightarrow \\quad w(X^\\top X)=yX \\quad \\Leftrightarrow \\quad (X^\\top X)w^\\top = X^\\top y^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14c104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.solve(X.T@X, X.T@y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77c5209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52635202, -4.02713947,  3.6416331 , -6.53016208])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038c189",
   "metadata": {},
   "source": [
    "It's Wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4405c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros((X.shape[0], X.shape[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e9955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X.shape[0]):\n",
    "    temp = list(X[i])\n",
    "    temp.insert(0, 1)  #List.insert(index, value) index에 value값을 넣기\n",
    "    Z[i] = np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffd4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.solve(Z.T@Z, Z.T@y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb3e16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.99759257,  2.96583528, -4.02713947,  0.98530689, -6.99427012])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374de675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd17d355",
   "metadata": {},
   "source": [
    "### Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce243fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=4\n",
    "OUTPUT_DIM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50dc24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights):\n",
    "    pred = np.matmul(weights, X.T)\n",
    "    return pred\n",
    "\n",
    "def MSE(X, y, pred):\n",
    "    N = X.shape[0]\n",
    "    loss = np.sum((pred-y)**2) / (2*N)\n",
    "    return loss\n",
    "\n",
    "def compute_grads(X, y, pred):\n",
    "    N     = X.shape[0]\n",
    "    grads = (1/N)*(-np.matmul(y,X) + np.matmul(pred, X))\n",
    "    return grads\n",
    "\n",
    "def update_weights(weights, grads, LR):\n",
    "    weights -= LR*grads\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3980ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 30\n",
    "EPOCHS=100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f5c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= np.random.randn(OUTPUT_DIM,INPUT_DIM+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd8ed066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 Completed, Loss: 58.621\n",
      "EPOCH 2 Completed, Loss: 51.648\n",
      "EPOCH 3 Completed, Loss: 24.449\n",
      "EPOCH 4 Completed, Loss: 9.209\n",
      "EPOCH 5 Completed, Loss: 16.684\n",
      "EPOCH 6 Completed, Loss: 8.213\n",
      "EPOCH 7 Completed, Loss: 11.976\n",
      "EPOCH 8 Completed, Loss: 12.771\n",
      "EPOCH 9 Completed, Loss: 7.629\n",
      "EPOCH 10 Completed, Loss: 8.944\n",
      "EPOCH 11 Completed, Loss: 7.465\n",
      "EPOCH 12 Completed, Loss: 8.761\n",
      "EPOCH 13 Completed, Loss: 6.616\n",
      "EPOCH 14 Completed, Loss: 5.823\n",
      "EPOCH 15 Completed, Loss: 6.008\n",
      "EPOCH 16 Completed, Loss: 5.483\n",
      "EPOCH 17 Completed, Loss: 3.970\n",
      "EPOCH 18 Completed, Loss: 8.273\n",
      "EPOCH 19 Completed, Loss: 4.767\n",
      "EPOCH 20 Completed, Loss: 4.130\n",
      "EPOCH 21 Completed, Loss: 3.607\n",
      "EPOCH 22 Completed, Loss: 3.349\n",
      "EPOCH 23 Completed, Loss: 3.899\n",
      "EPOCH 24 Completed, Loss: 2.240\n",
      "EPOCH 25 Completed, Loss: 2.446\n",
      "EPOCH 26 Completed, Loss: 4.168\n",
      "EPOCH 27 Completed, Loss: 2.777\n",
      "EPOCH 28 Completed, Loss: 2.708\n",
      "EPOCH 29 Completed, Loss: 3.225\n",
      "EPOCH 30 Completed, Loss: 4.419\n",
      "EPOCH 31 Completed, Loss: 2.592\n",
      "EPOCH 32 Completed, Loss: 2.159\n",
      "EPOCH 33 Completed, Loss: 2.162\n",
      "EPOCH 34 Completed, Loss: 3.570\n",
      "EPOCH 35 Completed, Loss: 2.076\n",
      "EPOCH 36 Completed, Loss: 2.802\n",
      "EPOCH 37 Completed, Loss: 2.467\n",
      "EPOCH 38 Completed, Loss: 3.014\n",
      "EPOCH 39 Completed, Loss: 2.011\n",
      "EPOCH 40 Completed, Loss: 2.262\n",
      "EPOCH 41 Completed, Loss: 2.133\n",
      "EPOCH 42 Completed, Loss: 2.516\n",
      "EPOCH 43 Completed, Loss: 1.879\n",
      "EPOCH 44 Completed, Loss: 2.623\n",
      "EPOCH 45 Completed, Loss: 2.840\n",
      "EPOCH 46 Completed, Loss: 2.232\n",
      "EPOCH 47 Completed, Loss: 1.664\n",
      "EPOCH 48 Completed, Loss: 2.324\n",
      "EPOCH 49 Completed, Loss: 1.491\n",
      "EPOCH 50 Completed, Loss: 3.477\n",
      "EPOCH 51 Completed, Loss: 1.579\n",
      "EPOCH 52 Completed, Loss: 1.911\n",
      "EPOCH 53 Completed, Loss: 3.183\n",
      "EPOCH 54 Completed, Loss: 2.388\n",
      "EPOCH 55 Completed, Loss: 2.123\n",
      "EPOCH 56 Completed, Loss: 2.826\n",
      "EPOCH 57 Completed, Loss: 1.750\n",
      "EPOCH 58 Completed, Loss: 2.145\n",
      "EPOCH 59 Completed, Loss: 2.052\n",
      "EPOCH 60 Completed, Loss: 2.654\n",
      "EPOCH 61 Completed, Loss: 2.597\n",
      "EPOCH 62 Completed, Loss: 1.550\n",
      "EPOCH 63 Completed, Loss: 2.111\n",
      "EPOCH 64 Completed, Loss: 1.520\n",
      "EPOCH 65 Completed, Loss: 1.853\n",
      "EPOCH 66 Completed, Loss: 2.416\n",
      "EPOCH 67 Completed, Loss: 2.723\n",
      "EPOCH 68 Completed, Loss: 1.321\n",
      "EPOCH 69 Completed, Loss: 1.750\n",
      "EPOCH 70 Completed, Loss: 2.800\n",
      "EPOCH 71 Completed, Loss: 1.882\n",
      "EPOCH 72 Completed, Loss: 1.952\n",
      "EPOCH 73 Completed, Loss: 1.401\n",
      "EPOCH 74 Completed, Loss: 1.224\n",
      "EPOCH 75 Completed, Loss: 1.997\n",
      "EPOCH 76 Completed, Loss: 1.266\n",
      "EPOCH 77 Completed, Loss: 2.388\n",
      "EPOCH 78 Completed, Loss: 2.111\n",
      "EPOCH 79 Completed, Loss: 2.599\n",
      "EPOCH 80 Completed, Loss: 2.688\n",
      "EPOCH 81 Completed, Loss: 2.041\n",
      "EPOCH 82 Completed, Loss: 1.097\n",
      "EPOCH 83 Completed, Loss: 1.004\n",
      "EPOCH 84 Completed, Loss: 1.073\n",
      "EPOCH 85 Completed, Loss: 1.468\n",
      "EPOCH 86 Completed, Loss: 1.761\n",
      "EPOCH 87 Completed, Loss: 1.643\n",
      "EPOCH 88 Completed, Loss: 1.948\n",
      "EPOCH 89 Completed, Loss: 1.305\n",
      "EPOCH 90 Completed, Loss: 1.493\n",
      "EPOCH 91 Completed, Loss: 2.898\n",
      "EPOCH 92 Completed, Loss: 1.812\n",
      "EPOCH 93 Completed, Loss: 1.354\n",
      "EPOCH 94 Completed, Loss: 2.274\n",
      "EPOCH 95 Completed, Loss: 1.355\n",
      "EPOCH 96 Completed, Loss: 1.820\n",
      "EPOCH 97 Completed, Loss: 1.815\n",
      "EPOCH 98 Completed, Loss: 1.489\n",
      "EPOCH 99 Completed, Loss: 2.100\n",
      "EPOCH 100 Completed, Loss: 2.017\n",
      "Total time: 0:00:00.231241\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Shuffle Data\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    x_temp = Z[idx]\n",
    "    y_temp = y[idx]\n",
    "    \n",
    "    for batch in range(X.shape[0]//BATCH_SIZE):\n",
    "        batch_X = x_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        batch_y = y_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE].reshape(1,-1)\n",
    "        \n",
    "        pred  = forward(batch_X, weights)\n",
    "        loss  = MSE(batch_X, batch_y, pred)\n",
    "        grads = compute_grads(batch_X, batch_y, pred)\n",
    "        \n",
    "        weights = update_weights(weights, grads, LR)\n",
    "    \n",
    "    print('EPOCH %d Completed, Loss: %.3f' % (epoch+1, loss))\n",
    "    \n",
    "end = datetime.now()\n",
    "print('Total time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "019e6eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.90051296,  2.93835035, -4.02661454,  1.01599308, -6.9884521 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
