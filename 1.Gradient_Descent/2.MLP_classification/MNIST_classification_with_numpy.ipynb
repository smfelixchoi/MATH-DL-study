{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNrVAxaX9sDe"
   },
   "source": [
    "# MNIST Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Oeri9dh9cMvU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p_Zh-JrH6ZPY"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PakNnESl6i25",
    "outputId": "7672712d-7d34-498c-9e89-c5071b820108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (60000, 28, 28) , Training Label: (60000,)\n",
      "Test Data: (10000, 28, 28) , Test Label: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Data:', x_train.shape, ',', 'Training Label:', y_train.shape)\n",
    "print('Test Data:', x_test.shape, ',', 'Test Label:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYYOW3Li93lo"
   },
   "source": [
    "## Preprocessing Dataset\n",
    "\n",
    "- Divide Training set into **training and validation set**.\n",
    "- Flatten Image Data: (28,28) -> (784,)\n",
    "- One-hot encode labels: 3 -> (0,0,0,1,0,0,0,0,0,0)\n",
    "- Normalize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = x_train[:50000], x_train[50000:]\n",
    "y_train, y_valid = y_train[:50000], y_train[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "USW5-lAr6ooP"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(50000,28*28)/256\n",
    "x_valid = x_valid.reshape(-1, 28*28)/256\n",
    "x_test  = x_test.reshape(-1, 28*28)/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FItW6ZAl7Obd",
    "outputId": "9327ae1b-baaf-42f8-c609-5a09ce9fffea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OU-WpSFh8a2V"
   },
   "outputs": [],
   "source": [
    "y_train_one_hot = np.eye(10)[y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7ygFBhg-nvB"
   },
   "source": [
    "# Define 2-Layer MLP\n",
    "\n",
    "- Number of Nodes\n",
    "    - Input dimension: 784\n",
    "    - Hidden dimension: 100 (Hyperparameter)\n",
    "    - Output dimension: 10 \n",
    "\n",
    "- Activation Functions\n",
    "    - Sigmoid\n",
    "    - ReLU\n",
    "\n",
    "- Loss Function: Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qDyRxrmzeBnT"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM=28*28\n",
    "HIDDEN_DIM=100\n",
    "OUTPUT_DIM=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_4F--PMcY98"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "- Sigmoid Function:\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "- ReLU:\n",
    "$$ ReLU(z) = \\max(0, z) = \\begin{cases} z & \\text{if }\\, z \\geq 0 \\\\ 0 & \\text{if }\\, z < 0 \\end{cases}.$$\n",
    "\n",
    "- Softmax: if $z = (z_1, \\dots, z_K)$, then $\\mathrm{softmax}(z) = [p_1, \\dots, p_K]$ where\n",
    "$$ p_j = \\frac{e^{z_j}}{\\sum_{i=1}^K e^{z_i}}$$\n",
    "for $j=1,\\dots, K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EQgXvfZecVcp"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1.+ np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def raw_softmax(Z):\n",
    "    e = np.exp(Z)\n",
    "    p = e/np.sum(e, axis=1, keepdims=True)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cz06CHNOiRFl"
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    maxes = np.max(Z, axis=1, keepdims=True)\n",
    "    e = np.exp(Z-maxes)\n",
    "    p = e / np.sum(e, axis=1, keepdims=True)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO8ou_tb_QL0"
   },
   "source": [
    "## Initialize Weights\n",
    "\n",
    "- Sigmoid Function\n",
    "    - Xavier Normal/Uniform\n",
    "$$ w_{ij} \\sim N\\left(0, \\sqrt{\\frac{2}{n_{in}+n_{out}}}^2 \\right) \\quad \\text{or} \\quad w_{ij} \\sim \\mathrm{Unif}\\left(-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\;\\sqrt{\\frac{6}{n_{in}+n_{out}}}\\right)$$\n",
    "\n",
    "- ReLU Function\n",
    "    - He Uniform/Normal\n",
    "$$ w_{ij} \\sim N\\left(0, \\sqrt{\\frac{2}{n_{in}}}^2 \\right) \\quad \\text{or} \\quad w_{ij} \\sim \\mathrm{Unif}\\left(-\\sqrt{\\frac{6}{n_{in}}}, \\;\\sqrt{\\frac{6}{n_{in}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bWdD3oPVdlaQ"
   },
   "outputs": [],
   "source": [
    "Params = {'W1': np.random.randn(INPUT_DIM, HIDDEN_DIM),\n",
    "          'b1': np.zeros((1, HIDDEN_DIM)),\n",
    "          'W2': np.random.randn(HIDDEN_DIM, OUTPUT_DIM),\n",
    "          'b2': np.zeros((1, OUTPUT_DIM))\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EUWAW96Pe-Yq"
   },
   "outputs": [],
   "source": [
    "def forward_NN(X, params, activation):\n",
    "    forwardpass = {}\n",
    "    forwardpass['Z1'] = np.matmul(X, params['W1']) + params['b1']\n",
    "    forwardpass['A'] = activation(forwardpass['Z1'])\n",
    "    forwardpass['Z2'] = np.matmul(forwardpass['A'], params['W2']) + params['b2']\n",
    "    forwardpass['Y_hat'] = softmax(forwardpass['Z2'])\n",
    "    return forwardpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpZy5NsQml0b"
   },
   "source": [
    "## Categorical Cross Entropy Loss for $K$ classes\n",
    "\n",
    "$$L(\\theta) = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{j=1}^K y_j^n \\log(\\hat{y}_j^n)$$\n",
    "\n",
    "where $\\hat{y}_j^n = h_\\theta(x^n)_j$. \n",
    "\n",
    "Note that, if the data $x^n$ belongs to $j_\\ast$th class, then \n",
    "$$ y_j^n = \\begin{cases} 1 & \\text{if} \\, j = j_\\ast \\\\ 0 & \\text{if} \\, j \\neq j_\\ast \\end{cases}$$\n",
    "due to one-hot encoding.\n",
    "\n",
    "That is, the loss is \n",
    "$$L(\\theta) = -\\frac{1}{N} \\sum_{n=1}^N y_{j_\\ast}^n \\log(\\hat{y}_{j_\\ast}^n).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9YpSBdothQty"
   },
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(y, y_hat):\n",
    "    # y: true label (one-hot code)\n",
    "    # y_hat: predicted probability\n",
    "    # Batch Size: N\n",
    "    N = y.shape[0]\n",
    "    loss = -(1/N) * np.sum(y * np.log(y_hat + 1e-5))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDndX7tQAw9l"
   },
   "source": [
    "## L2 Regularization\n",
    "\n",
    "Regularization to reduce the complexity of the network.\n",
    "\n",
    "$\\lambda$: weight_decay parameter\n",
    "\n",
    "$$L2(w) = \\frac{\\lambda}{2} \\sum_{i=1}^M w_i^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "G0yu9GjIjPtm"
   },
   "outputs": [],
   "source": [
    "def L2_regularization(params, weight_decay):\n",
    "    # weight decay (lambda): small positive number\n",
    "    sum = 0\n",
    "    sum += np.sum(params['W1']**2)\n",
    "    sum += np.sum(params['b1']**2)\n",
    "    sum += np.sum(params['W2']**2)\n",
    "    sum += np.sum(params['b2']**2)\n",
    "    reg = (weight_decay/2)*sum\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axjYvEg3DG-0"
   },
   "source": [
    "## Gradient Descent \n",
    "\n",
    "- Update Equation:\n",
    "$$ w \\leftarrow w - \\eta \\cdot \\nabla_w L(w)$$\n",
    "\n",
    "\n",
    "- If we use L2 regularization, then the update equation is the following: $ w \\leftarrow w - \\eta \\cdot \\nabla_w [L(w) + L2(w)]$\n",
    "\n",
    "- **Gradients with respect to weights are computed using CHAIN RULE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "j-9636IegGyO"
   },
   "outputs": [],
   "source": [
    "def backward(X, y, forwardpass, params, activation):\n",
    "    '''\n",
    "    grads['Z2']: (batch, OUTPUT_DIM)\n",
    "    grads['W2']: (HIDDEN_DIM, OUTPUT_DIM)\n",
    "    grads['b2']: (1, OUTPUT_DIM)\n",
    "    grads['A'] : (batch, HIDDEN_DIM)\n",
    "    grads['Z1']: (batch, HIDDEN_DIM)\n",
    "    grads['W1']: (INPUT_DIM, HIDDEN_DIM)\n",
    "    grads['b1']: (1, HIDDEN_DIM)\n",
    "    X :          (batch, INPUT_DIM)\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    grads={}\n",
    "    grads['Z2'] = (1/N) * (forwardpass['Y_hat'] - y)\n",
    "    grads['W2'] = np.sum(np.matmul( forwardpass['A'].reshape(N,-1,1), grads['Z2'].reshape(N,1,-1)), \n",
    "                         axis=0, keepdims=False)\n",
    "    grads['b2'] = np.sum(grads['Z2'], axis=0, keepdims=True)\n",
    "    grads['A']  = np.matmul(grads['Z2'], params['W2'].T)\n",
    "\n",
    "    if activation == sigmoid:\n",
    "        grads['Z1'] = grads['A'] * forwardpass['A'] * (1-forwardpass['A'])\n",
    "        \n",
    "    elif activation == relu:\n",
    "        grads['Z1'] = grads['A'] * (forwardpass['A']>0)\n",
    "    \n",
    "    grads['W1'] = np.sum( np.matmul(X.reshape(N,-1,1), grads['Z1'].reshape(N,1,-1)), axis=0, keepdims=False)\n",
    "    grads['b1'] = np.sum( grads['Z1'], axis=0, keepdims=True)\n",
    "\n",
    "    return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "I5u7Z197gWRV"
   },
   "outputs": [],
   "source": [
    "def update_params(params, grads, eta, weight_decay):\n",
    "    params['W1'] -= eta * ( grads['W1'] + weight_decay*params['W1'] )\n",
    "    params['b1'] -= eta * ( grads['b1'] + weight_decay*params['b1'] )\n",
    "    params['W2'] -= eta * ( grads['W2'] + weight_decay*params['W2'] )\n",
    "    params['b2'] -= eta * ( grads['b2'] + weight_decay*params['b2'] )\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEl1HO2GD9oN"
   },
   "source": [
    "## Train the Neural Network\n",
    "\n",
    "WHAT SHOULD BE SET?\n",
    "- Batch Size\n",
    "- Number of Epochs\n",
    "- Learning Rate $\\eta$\n",
    "- Weight Decay $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "r-9a2G_Q_1TH"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=250\n",
    "EPOCHS=20\n",
    "LR = 0.01\n",
    "WEIGHT_DECAY = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Used for validation check or evaluation check\n",
    "\n",
    "def predict(X, y, params, activation):\n",
    "    prob = forward_NN(X, params, activation)['Y_hat']\n",
    "    pred = np.argmax(prob, axis=1)\n",
    "    acc = np.sum(pred == y) / y.shape[0]\n",
    "    return pred, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "o0euhNA8EzXz"
   },
   "outputs": [],
   "source": [
    "sigmoidParams = {'W1': np.random.randn(INPUT_DIM, HIDDEN_DIM), \n",
    "                 'b1': np.zeros((1, HIDDEN_DIM)),\n",
    "                 'W2': np.random.randn(HIDDEN_DIM, OUTPUT_DIM),\n",
    "                 'b2': np.zeros((1, OUTPUT_DIM))\n",
    "                }\n",
    "\n",
    "losses_sigmoid = np.zeros(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "63zKul1CErWI",
    "outputId": "e9d43d69-2bbc-4a74-cadb-67741fba65ae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 Completed, Loss: 8.394, Accuracy: 0.2100\n",
      "EPOCH 2 Completed, Loss: 7.465, Accuracy: 0.2813\n",
      "EPOCH 3 Completed, Loss: 7.237, Accuracy: 0.3440\n",
      "EPOCH 4 Completed, Loss: 6.719, Accuracy: 0.3963\n",
      "EPOCH 5 Completed, Loss: 6.178, Accuracy: 0.4383\n",
      "EPOCH 6 Completed, Loss: 5.991, Accuracy: 0.4772\n",
      "EPOCH 7 Completed, Loss: 5.872, Accuracy: 0.5064\n",
      "EPOCH 8 Completed, Loss: 5.799, Accuracy: 0.5354\n",
      "EPOCH 9 Completed, Loss: 6.021, Accuracy: 0.5606\n",
      "EPOCH 10 Completed, Loss: 5.740, Accuracy: 0.5801\n",
      "EPOCH 11 Completed, Loss: 5.477, Accuracy: 0.5980\n",
      "EPOCH 12 Completed, Loss: 5.424, Accuracy: 0.6122\n",
      "EPOCH 13 Completed, Loss: 5.395, Accuracy: 0.6266\n",
      "EPOCH 14 Completed, Loss: 5.257, Accuracy: 0.6399\n",
      "EPOCH 15 Completed, Loss: 5.384, Accuracy: 0.6507\n",
      "EPOCH 16 Completed, Loss: 5.225, Accuracy: 0.6630\n",
      "EPOCH 17 Completed, Loss: 5.053, Accuracy: 0.6725\n",
      "EPOCH 18 Completed, Loss: 4.971, Accuracy: 0.6821\n",
      "EPOCH 19 Completed, Loss: 5.205, Accuracy: 0.6905\n",
      "EPOCH 20 Completed, Loss: 5.124, Accuracy: 0.6979\n",
      "Total time: 0:06:43.401664\n"
     ]
    }
   ],
   "source": [
    "## Sigmoid Activation\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Shuffle Data\n",
    "    idx = np.random.permutation(50000)\n",
    "    x_temp = x_train[idx]\n",
    "    y_temp = y_train_one_hot[idx]\n",
    "    \n",
    "    for batch in range(x_train.shape[0]//BATCH_SIZE):\n",
    "        X = x_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        y = y_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        \n",
    "        forwardpass = forward_NN(X, sigmoidParams, sigmoid)\n",
    "        loss = CrossEntropyLoss(y, forwardpass['Y_hat']) + L2_regularization(sigmoidParams, WEIGHT_DECAY)\n",
    "        grads = backward(X, y, forwardpass, sigmoidParams, sigmoid)\n",
    "        \n",
    "        sigmoidParams = update_params(sigmoidParams, grads, LR, WEIGHT_DECAY)\n",
    "    \n",
    "    losses_sigmoid[epoch] = loss\n",
    "    pred, valid_acc = predict(x_valid, y_valid, sigmoidParams, sigmoid)\n",
    "    \n",
    "    print('EPOCH %d Completed, Loss: %.3f, Accuracy: %.4f' % (epoch+1, loss, valid_acc))\n",
    "\n",
    "end = datetime.now()\n",
    "print('Total time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BqLH0GmAi5Xh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Test Accuracy: 70.38 %\n"
     ]
    }
   ],
   "source": [
    "pred, test_acc = predict(x_test, y_test, sigmoidParams, sigmoid)\n",
    "print('Sigmoid Test Accuracy:', test_acc*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network wit ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLUParams = {'W1': np.random.randn(INPUT_DIM, HIDDEN_DIM), \n",
    "                 'b1': np.zeros((1, HIDDEN_DIM)),\n",
    "                 'W2': np.random.randn(HIDDEN_DIM, OUTPUT_DIM),\n",
    "                 'b2': np.zeros((1, OUTPUT_DIM))\n",
    "                }\n",
    "\n",
    "losses_relu = np.zeros(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 Completed, Loss: 8.682, Accuracy: 0.5549\n",
      "EPOCH 2 Completed, Loss: 7.757, Accuracy: 0.6757\n",
      "EPOCH 3 Completed, Loss: 7.083, Accuracy: 0.7295\n",
      "EPOCH 4 Completed, Loss: 5.860, Accuracy: 0.7590\n",
      "EPOCH 5 Completed, Loss: 6.743, Accuracy: 0.7809\n",
      "EPOCH 6 Completed, Loss: 5.893, Accuracy: 0.7919\n",
      "EPOCH 7 Completed, Loss: 5.741, Accuracy: 0.8036\n",
      "EPOCH 8 Completed, Loss: 5.820, Accuracy: 0.8141\n",
      "EPOCH 9 Completed, Loss: 5.475, Accuracy: 0.8226\n",
      "EPOCH 10 Completed, Loss: 5.544, Accuracy: 0.8250\n",
      "EPOCH 11 Completed, Loss: 5.821, Accuracy: 0.8308\n",
      "EPOCH 12 Completed, Loss: 5.351, Accuracy: 0.8360\n",
      "EPOCH 13 Completed, Loss: 5.216, Accuracy: 0.8405\n",
      "EPOCH 14 Completed, Loss: 5.437, Accuracy: 0.8433\n",
      "EPOCH 15 Completed, Loss: 5.304, Accuracy: 0.8449\n",
      "EPOCH 16 Completed, Loss: 5.329, Accuracy: 0.8495\n",
      "EPOCH 17 Completed, Loss: 5.355, Accuracy: 0.8526\n",
      "EPOCH 18 Completed, Loss: 5.268, Accuracy: 0.8538\n",
      "EPOCH 19 Completed, Loss: 5.231, Accuracy: 0.8574\n",
      "EPOCH 20 Completed, Loss: 4.763, Accuracy: 0.8576\n",
      "Total time: 0:06:43.676630\n"
     ]
    }
   ],
   "source": [
    "## ReLU Activation\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    idx = np.random.permutation(50000)\n",
    "    x_temp = x_train[idx]\n",
    "    y_temp = y_train_one_hot[idx]\n",
    "    \n",
    "    for batch in range(x_train.shape[0]//BATCH_SIZE):\n",
    "        X = x_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        y = y_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        \n",
    "        forwardpass = forward_NN(X, ReLUParams, relu)\n",
    "        loss = CrossEntropyLoss(y, forwardpass['Y_hat']) + L2_regularization(ReLUParams, WEIGHT_DECAY)\n",
    "        grads = backward(X, y, forwardpass, ReLUParams, relu)\n",
    "        \n",
    "        ReLUParams = update_params(ReLUParams, grads, LR, WEIGHT_DECAY)\n",
    "    \n",
    "    losses_relu[epoch] = loss    \n",
    "    pred, valid_acc = predict(x_valid, y_valid, ReLUParams, relu)\n",
    "    \n",
    "    print('EPOCH %d Completed, Loss: %.3f, Accuracy: %.4f' % (epoch+1, loss, valid_acc))\n",
    "\n",
    "end = datetime.now()\n",
    "print('Total time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Test Accuracy: 84.95 %\n"
     ]
    }
   ],
   "source": [
    "pred, test_acc = predict(x_test, y_test, ReLUParams, relu)\n",
    "print('ReLU Test Accuracy:', test_acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
